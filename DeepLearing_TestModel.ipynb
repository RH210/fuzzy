{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7803, 0.9722, 0.2464],\n",
      "        [0.1214, 0.2834, 0.9190],\n",
      "        [0.9813, 0.6690, 0.4189],\n",
      "        [0.1796, 0.5934, 0.0419],\n",
      "        [0.3943, 0.0094, 0.0644]])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (layer1): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
    "import torch.nn as nn  # 「ニューラルネットワーク」モジュールの別名定義\n",
    "\n",
    "# 定数（モデル定義時に必要となるもの）\n",
    "INPUT_FEATURES = 2  # 入力（特徴）の数： 2\n",
    "OUTPUT_NEURONS = 1  # ニューロンの数： 1\n",
    "\n",
    "# 変数（モデル定義時に必要となるもの）\n",
    "activation = torch.nn.Tanh()  # 活性化関数： tanh関数\n",
    "\n",
    "# 「torch.nn.Moduleクラスのサブクラス化」によるモデルの定義\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # 層（layer：レイヤー）を定義\n",
    "        self.layer1 = nn.Linear(  # Linearは「全結合層」を指す\n",
    "            INPUT_FEATURES,       # データ（特徴）の入力ユニット数\n",
    "            OUTPUT_NEURONS)       # 出力結果への出力ユニット数\n",
    "\n",
    "    def forward(self, input):\n",
    "        # フォワードパスを定義\n",
    "        output = activation(self.layer1(input))  # 活性化関数は変数として定義\n",
    "        # 「出力＝活性化関数（第n層（入力））」の形式で記述する。\n",
    "        # 層（layer）を重ねる場合は、同様の記述を続ければよい（第3回＝後述）。\n",
    "        # 「出力（output）」は次の層（layer）への「入力（input）」に使う。\n",
    "        # 慣例では入力も出力も「x」と同じ変数名で記述する（よって以下では「x」と書く）\n",
    "        return output\n",
    "\n",
    "# モデル（NeuralNetworkクラス）のインスタンス化\n",
    "model = NeuralNetwork()\n",
    "model   # モデルの内容を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight', tensor([[ 0.6000, -0.2000]])),\n",
       "             ('layer1.bias', tensor([0.8000]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パラメーター（ニューロンへの入力で必要となるもの）の定義\n",
    "weight_array = nn.Parameter(\n",
    "    torch.tensor([[ 0.6,\n",
    "                   -0.2]]))  # 重み\n",
    "bias_array = nn.Parameter(\n",
    "    torch.tensor([  0.8 ]))  # バイアス\n",
    "\n",
    "# 重みとバイアスの初期値設定\n",
    "model.layer1.weight = weight_array\n",
    "model.layer1.bias = bias_array\n",
    "\n",
    "# torch.nn.Module全体の状態を辞書形式で取得\n",
    "params = model.state_dict()\n",
    "#params = list(model.parameters()) # このように取得することも可能\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)  # 今回は入力に勾配（gradient）を必要とする\n",
    "# 「requires_grad」が「True」（デフォルト：False）の場合、\n",
    "# torch.autogradが入力テンソルに関するパラメーター操作（勾配）を記録するようになる\n",
    "\n",
    "#x.requires_grad_(True)  # 「requires_grad_()」メソッドで後から変更することも可能\n",
    "\n",
    "y = x ** 2     # 「yイコールxの二乗」という計算式の計算グラフを構築\n",
    "print(y)       # tensor(1., grad_fn=<PowBackward0>) ……などと表示される\n",
    "\n",
    "y.backward()   # 逆伝播の処理として、上記式から微分係数（＝勾配）を計算（自動微分：Autograd）\n",
    "\n",
    "g = x.grad     # 与えられた入力（x）によって計算された勾配の値（grad）を取得\n",
    "print(g)       # tensor(2.)  ……などと表示される\n",
    "# 計算式の微分係数（＝勾配）を計算するための導関数は「dy/dx=2x」なので、\n",
    "#「x=1.0」地点の勾配（＝接線の傾き）は「2.0」となり、出力結果は正しい。\n",
    "# 例えば「x=0.0」地点の勾配は「0.0」、「x=10.0」地点の勾配は「20.0」である"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2002, -0.4005]])\n",
      "tensor([-0.2002])\n"
     ]
    }
   ],
   "source": [
    "# 勾配計算の前に、各パラメーター（重みやバイアス）の勾配の値（grad）をリセットしておく\n",
    "model.layer1.weight.grad = None      # 重み\n",
    "model.layer1.bias.grad = None        # バイアス\n",
    "#model.zero_grad()                   # これを呼び出しても上記と同じくリセットされる\n",
    "\n",
    "X_data = torch.tensor([[1.0, 2.0]])  # 入力データ（※再掲）\n",
    "y_pred = model(X_data)               # 出力結果（※再掲）\n",
    "y_true = torch.tensor([[1.0]])       # 正解ラベル\n",
    "\n",
    "criterion = nn.MSELoss()             # 誤差からの損失を測る「基準」＝損失関数\n",
    "loss = criterion(y_pred, y_true)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n",
    "loss.backward()   # 逆伝播の処理として、勾配を計算（自動微分：Autograd）\n",
    "\n",
    "# 勾配の値（grad）は、各パラメーター（重みやバイアス）から取得できる\n",
    "print(model.layer1.weight.grad) # tensor([[-0.2002, -0.4005]])  ……などと表示される\n",
    "print(model.layer1.bias.grad)   # tensor([-0.2002])  ……などと表示される\n",
    "# ※パラメーターは「list(model.parameters())」で取得することも可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# テンソルの新規作成\n",
    "x = torch.empty(2, 3) # 2行×3列のテンソル（未初期化状態）を生成\n",
    "x = torch.rand(2, 3)  # 2行×3列のテンソル（ランダムに初期化）を生成\n",
    "x = torch.zeros(2, 3, dtype=torch.float) # 2行×3列のテンソル（0で初期化、torch.float型）を生成\n",
    "x = torch.ones(2, 3, dtype=torch.float)  # 2行×3列のテンソル（1で初期化、torch.float型）を生成\n",
    "x = torch.tensor([[0.0, 0.1, 0.2],\n",
    "                  [1.0, 1.1, 1.2]])      # 1行×2列のテンソルをPythonリスト値から作成\n",
    "\n",
    "# 既存のテンソルを使った新規作成\n",
    "# 「new_*()」パターン\n",
    "y = x.new_ones(2, 3)   # 2行×3列のテンソル（1で初期化、既存のテンソルと「同じデータ型」）を生成\n",
    "# 「*_like()」パターン # 既存のテンソルと「同じサイズ」のテンソル（1で初期化、torch.int型）を生成\n",
    "y = torch.ones_like(x, dtype=torch.int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンソルサイズの取得\n",
    "x.size()               # 「torch.Size([2, 3])」のように、2行3列と出力される\n",
    "x.shape                # NumPy風の記述も可能。出力は上と同じ\n",
    "len(x)   # 行数（＝データ数）を取得する際も、NumPy風に記述することが可能\n",
    "x.ndim   # テンソルの次元数を取得する際も、NumPy風に記述が可能\n",
    "\n",
    "# テンソルのサイズ変更／形状変更\n",
    "z = x.view(3, 2)       # 3行2列に変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 2.1000, 2.2000],\n",
       "        [3.0000, 3.1000, 3.2000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テンソルの計算操作\n",
    "x + y                  # 演算子を使う方法\n",
    "torch.add(x, y)        # 関数を使う方法\n",
    "torch.add(x, y, out=x) # outパラメーターで出力先の変数を指定可能\n",
    "x.add_(y)              # 「*_()」パターン。xを置き換えて出力する例（上記のコードと同じ処理）\n",
    "# PyTorchでは、メソッド名の最後にアンダースコア（_）がある場合（例えば「add_()」）、「テンソルの内部置き換え（in-place changes）が起こること」を意味する。\n",
    "# アンダースコア（_）がない通常の計算の場合（例えば「add()」）は、計算元のテンソル内部は変更されずに、戻り値として新たなテンソルが取得できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 2.1000, 2.2000],\n",
      "        [3.0000, 3.1000, 3.2000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.1000, 2.2000],\n",
       "        [3.1000, 3.2000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# インデクシングやスライシング（NumPyのような添え字を使用可能）\n",
    "print(x)         # 元は、2行3列のテンソル\n",
    "x[0, 1]          # 1行2列目（※0スタート）を取得\n",
    "x[:2, 1:]        # 先頭～2行（＝0行目と1行目）×2列～末尾（＝2列目と3列目）の2行2列が抽出される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0999999046325684"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テンソルの1つの要素値を、Pythonの数値に変換\n",
    "x[0, 1].item()   # 1行2列目（※0スタート）の要素値をPythonの数値で取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch計算→NumPy反映：\n",
      "[[2.  2.1 2.2]\n",
      " [3.  3.1 3.2]]\n",
      "[[3.  3.1 3.2]\n",
      " [4.  4.1 4.2]]\n",
      "NumPy計算→PyTorch反映：\n",
      "tensor([[3.0000, 3.1000, 3.2000],\n",
      "        [4.0000, 4.1000, 4.2000]])\n",
      "tensor([[6.0000, 6.2000, 6.4000],\n",
      "        [8.0000, 8.2000, 8.4000]])\n",
      "NumPy計算→PyTorch反映：\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]], dtype=torch.float64)\n",
      "PyTorch計算→NumPy反映：\n",
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]]\n",
      "[[3. 3. 3.]\n",
      " [3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "# PyTorchテンソルを、NumPy多次元配列に変換\n",
    "b = x.numpy()    # 「numpy()」を呼び出すだけ。以下は注意点（メモリ位置の共有）\n",
    "\n",
    "# ※PyTorchテンソル側の値を変えると、NumPy多次元配列値「b」も変化する（トラックされる）\n",
    "print ('PyTorch計算→NumPy反映：')\n",
    "print(b); x.add_(y); print(b)           # PyTorch側の計算はNumPy側に反映\n",
    "print ('NumPy計算→PyTorch反映：')\n",
    "print(x); np.add(b, b, out=b); print(x) # NumPy側の計算はPyTorch側に反映\n",
    "\n",
    "# -----------------------------------------\n",
    "# NumPy多次元配列を、PyTorchテンソルに変換\n",
    "c = np.ones((2, 3), dtype=np.float64) # 2行3列の多次元配列値（1で初期化）を生成\n",
    "d = torch.from_numpy(c)  # 「torch.from_numpy()」を呼び出すだけ\n",
    "\n",
    "# ※NumPy多次元配列値を変えると、PyTorchテンソル「d」も変化する（トラックされる）\n",
    "print ('NumPy計算→PyTorch反映：')\n",
    "print(d); np.add(c, c, out=c); print(d)  # NumPy側の計算はPyTorch側に反映\n",
    "print ('PyTorch計算→NumPy反映：')\n",
    "print(c); d.add_(y); print(c)            # PyTorch側の計算はNumPy側に反映"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ型の変換（※変換後のテンソルには、NumPyの計算は反映されない）\n",
    "e = d.float()  # 「torch.float64」から「torch.float32」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA（GPU）が利用できない環境\n"
     ]
    }
   ],
   "source": [
    "# NVIDIAのGPUである「CUDA」（GPU）デバイス環境が利用可能な場合、\n",
    "# GPUを使ってテンソルの計算を行うこともできる\n",
    "if torch.cuda.is_available():              # CUDA（GPU）が利用可能な場合\n",
    "    print('CUDA（GPU）が利用できる環境')\n",
    "    print(f'CUDAデバイス数： {torch.cuda.device_count()}')\n",
    "    print(f'現在のCUDAデバイス番号： {torch.cuda.current_device()}')  # ※0スタート\n",
    "    print(f'1番目のCUDAデバイス名： {torch.cuda.get_device_name(0)}') # 例「Tesla T4」   \n",
    "\n",
    "    device = torch.device(\"cuda\")          # デフォルトのCUDAデバイスオブジェクトを取得\n",
    "    device0 = torch.device(\"cuda:0\")       # 1番目（※0スタート）のCUDAデバイスを取得\n",
    "\n",
    "    # テンソル計算でのGPUの使い方は主に3つ：\n",
    "    g = torch.ones(2, 3, device=device)    # （1）テンソル生成時のパラメーター指定\n",
    "    g = x.to(device)                       # （2）既存テンソルのデバイス変更\n",
    "    g = x.cuda(device)                     # （3）既存テンソルの「CUDA（GPU）」利用\n",
    "    f = x.cpu()                            # （3'）既存テンソルの「CPU」利用\n",
    "\n",
    "    # ※（2）の使い方で、GPUは「.to(\"cuda\")」、CPUは「.to(\"cpu\")」と書いてもよい\n",
    "    g = x.to(\"cuda\")\n",
    "    f = x.to(\"cpu\")\n",
    "\n",
    "    # ※（3）の引数は省略することも可能\n",
    "    g = x.cuda()\n",
    "\n",
    "    # 「torch.nn.Module」オブジェクト（model）全体でのGPU／CPUの切り替え\n",
    "    model.cuda()  # モデルの全パラメーターとバッファーを「CUDA（GPU）」に移行する\n",
    "    model.cpu()   # モデルの全パラメーターとバッファーを「CPU」に移行する\n",
    "else:\n",
    "    print('CUDA（GPU）が利用できない環境')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting playground-data\n",
      "  Downloading playground-data-1.1.1.tar.gz (19 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ryoha\\anaconda3\\lib\\site-packages (from playground-data) (1.19.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ryoha\\anaconda3\\lib\\site-packages (from playground-data) (3.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ryoha\\anaconda3\\lib\\site-packages (from matplotlib->playground-data) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\ryoha\\anaconda3\\lib\\site-packages (from matplotlib->playground-data) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\ryoha\\anaconda3\\lib\\site-packages (from matplotlib->playground-data) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ryoha\\anaconda3\\lib\\site-packages (from matplotlib->playground-data) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\ryoha\\anaconda3\\lib\\site-packages (from matplotlib->playground-data) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ryoha\\anaconda3\\lib\\site-packages (from matplotlib->playground-data) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\ryoha\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->playground-data) (1.15.0)\n",
      "Building wheels for collected packages: playground-data\n",
      "  Building wheel for playground-data (setup.py): started\n",
      "  Building wheel for playground-data (setup.py): finished with status 'done'\n",
      "  Created wheel for playground-data: filename=playground_data-1.1.1-py2.py3-none-any.whl size=20787 sha256=11f16133c470871dd9c4eb20ee45d571e9f633167cca34621850732b3ebf0605\n",
      "  Stored in directory: c:\\users\\ryoha\\appdata\\local\\pip\\cache\\wheels\\61\\f2\\36\\55780518e4385f4d3180bc710f66904cfc103daa5e074b8376\n",
      "Successfully built playground-data\n",
      "Installing collected packages: playground-data\n",
      "Successfully installed playground-data-1.1.1\n"
     ]
    }
   ],
   "source": [
    "# 座標点データを生成するライブラリのインストール\n",
    "!pip install playground-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "[[ 4.87362268  0.38445872]\n",
      " [-0.13254658  0.47962216]\n",
      " [ 0.35844454 -0.68676156]\n",
      " [ 0.18693752  1.89160769]\n",
      " [-3.08464514 -2.93898051]]\n",
      "y_train:\n",
      "[[-1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [-1.]]\n",
      "X_valid:\n",
      "[[ 0.21086285  0.32509686]\n",
      " [ 0.55111076 -0.06629613]\n",
      " [ 0.08933804  1.02348701]\n",
      " [ 2.06949344 -0.74432817]\n",
      " [-0.16874364 -0.06874526]]\n",
      "y_valid:\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# playground-dataライブラリのplygdataパッケージを「pg」という別名でインポート\n",
    "import plygdata as pg\n",
    "\n",
    "# 設定値を定数として定義\n",
    "PROBLEM_DATA_TYPE = pg.DatasetType.ClassifyCircleData # 問題種別：「分類（Classification）」、データ種別：「円（CircleData）」を選択\n",
    "TRAINING_DATA_RATIO = 0.5  # データの何％を訓練【Training】用に？ (残りは精度検証【Validation】用) ： 50％\n",
    "DATA_NOISE = 0.0           # ノイズ： 0％\n",
    "\n",
    "# 定義済みの定数を引数に指定して、データを生成する\n",
    "data_list = pg.generate_data(PROBLEM_DATA_TYPE, DATA_NOISE)\n",
    "\n",
    "# データを「訓練用」と「精度検証用」を指定の比率で分割し、さらにそれぞれを「データ（X）」と「教師ラベル（y）」に分ける\n",
    "X_train, y_train, X_valid, y_valid = pg.split_data(data_list, training_size=TRAINING_DATA_RATIO)\n",
    "\n",
    "# データ分割後の各変数の内容例として、それぞれ5件ずつ出力\n",
    "print('X_train:'); print(X_train[:5])\n",
    "print('y_train:'); print(y_train[:5])\n",
    "print('X_valid:'); print(X_valid[:5])\n",
    "print('y_valid:'); print(y_valid[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ関連のユーティリティクラスをインポート\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
    "\n",
    "# 定数（学習方法設計時に必要となるもの）\n",
    "BATCH_SIZE = 15  # バッチサイズ： 15（Playgroundの選択肢は「1」～「30」）\n",
    "\n",
    "# NumPy多次元配列からテンソルに変換し、データ型は`float`に変換する\n",
    "t_X_train = torch.from_numpy(X_train).float()\n",
    "t_y_train = torch.from_numpy(y_train).float()\n",
    "t_X_valid = torch.from_numpy(X_valid).float()\n",
    "t_y_valid = torch.from_numpy(y_valid).float()\n",
    "\n",
    "# 「データ（X）」と「教師ラベル（y）」を、1つの「データセット（dataset）」にまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)  # 訓練用\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)  # 精度検証用\n",
    "\n",
    "# ミニバッチを扱うための「データローダー（loader）」（訓練用と精度検証用）を作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  1.,  1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
    "import torch.nn as nn  # 「ニューラルネットワーク」モジュールの別名定義\n",
    "\n",
    "# 離散化を行う単なる関数\n",
    "def discretize(proba):\n",
    "    '''\n",
    "    実数の確率値を「1」か「-1」の2クラス分類値に離散化する。\n",
    "    閾値は「0.0以上」か「未満」か。データ型は「torch.float」を想定。\n",
    "  \n",
    "    Examples:\n",
    "        >>> proba = torch.tensor([-0.5, 0.0, 0.5], dtype=torch.float)\n",
    "        >>> binary = discretize(proba)\n",
    "    '''\n",
    "    threshold = torch.Tensor([0.0]) # -1か1かを分ける閾値を作成\n",
    "    discretized = (proba >= threshold).float() # 閾値未満で0、以上で1に変換\n",
    "    return discretized * 2 - 1.0 # 2倍して-1.0することで、0／1を-1.0／1.0にスケール変換\n",
    "\n",
    "# discretize関数をモデルで簡単に使用できるようにするため、\n",
    "# PyTorchの「torch.nn.Module」を継承したクラスラッパーも作成した\n",
    "class Discretize(nn.Module):\n",
    "    '''\n",
    "    実数の確率値を「1」か「-1」の2クラス分類値に離散化する。\n",
    "    閾値は「0.0以上」か「未満」か。データ型は「torch.float」を想定。\n",
    "  \n",
    "    Examples:\n",
    "        >>> d = Discretize()\n",
    "        >>> proba = torch.tensor([-0.5, 0.0, 0.5], dtype=torch.float)\n",
    "        >>> binary = d(proba)\n",
    "    '''        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # forward()メソッドは、基本クラス「torch.nn.Module」の__call__メソッドからも呼び出されるため、\n",
    "    # Discretizeオブジェクトを関数のように使える（例えば上記の「d(proba)」）\n",
    "    def forward(self, proba):\n",
    "        return discretize(proba) # 上記の関数を呼び出すだけ\n",
    "\n",
    "# 関数の利用をテスト\n",
    "proba = torch.tensor([-0.5, 0.0, 0.5], dtype=torch.float)  # 確率値の例\n",
    "binary = discretize(proba)  # 2クラス分類（binary classification）値に離散化\n",
    "binary  # tensor([-1.,  1.,  1.]) …… などと表示される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (layer1): Linear(in_features=2, out_features=3, bias=True)\n",
       "  (layer2): Linear(in_features=3, out_features=3, bias=True)\n",
       "  (layer_out): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch       # ライブラリ「PyTorch」のtorchパッケージをインポート\n",
    "import torch.nn as nn  # 「ニューラルネットワーク」モジュールの別名定義\n",
    "\n",
    "# 定数（モデル定義時に必要となるもの）\n",
    "INPUT_FEATURES = 2      # 入力（特徴）の数： 2\n",
    "LAYER1_NEURONS = 3      # ニューロンの数： 3\n",
    "LAYER2_NEURONS = 3      # ニューロンの数： 3\n",
    "OUTPUT_RESULTS = 1      # 出力結果の数： 1\n",
    "\n",
    "# 変数（モデル定義時に必要となるもの）\n",
    "activation1 = torch.nn.Tanh()  # 活性化関数（隠れ層用）： tanh関数（変更可能）\n",
    "activation2 = torch.nn.Tanh()  # 活性化関数（隠れ層用）： tanh関数（変更可能）\n",
    "acti_out = torch.nn.Tanh()     # 活性化関数（出力層用）： tanh関数（固定）\n",
    "\n",
    "# torch.nn.Moduleによるモデルの定義\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    # レイヤー（層）を定義\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # 隠れ層：1つ目のレイヤー（layer）\n",
    "        self.layer1 = nn.Linear(\n",
    "            INPUT_FEATURES,                # 入力ユニット数（＝入力層）\n",
    "            LAYER1_NEURONS)                # 次のレイヤーへの出力ユニット数\n",
    "\n",
    "        # 隠れ層：2つ目のレイヤー（layer）\n",
    "        self.layer2 = nn.Linear(\n",
    "            LAYER1_NEURONS,                # 入力ユニット数\n",
    "            LAYER2_NEURONS)                # 次のレイヤーへの出力ユニット数\n",
    "\n",
    "        # 出力層\n",
    "        self.layer_out = nn.Linear(\n",
    "            LAYER2_NEURONS,                # 入力ユニット数\n",
    "            OUTPUT_RESULTS)                # 出力結果への出力ユニット数\n",
    "\n",
    "    # フォワードパスを定義\n",
    "    def forward(self, x):\n",
    "        # 「出力＝活性化関数（第n層（入力））」の形式で記述\n",
    "        x = activation1(self.layer1(x))  # 活性化関数は変数として定義\n",
    "        x = activation2(self.layer2(x))  # 同上\n",
    "        x = acti_out(self.layer_out(x))  # ※活性化関数は「tanh」固定\n",
    "        return x\n",
    "\n",
    "# モデル（NeuralNetworkクラス）のインスタンス化\n",
    "model = NeuralNetwork()\n",
    "model   # モデルの内容を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim   # 「最適化」モジュールの別名定義\n",
    "\n",
    "# 定数（学習方法設計時に必要となるもの）\n",
    "LEARNING_RATE = 0.03   # 学習率： 0.03\n",
    "REGULARIZATION = 0.03  # 正則化率： 0.03\n",
    "\n",
    "# オプティマイザーを作成（パラメーターと学習率も指定）\n",
    "optimizer = optim.SGD(           # 最適化アルゴリズムに「SGD」を選択\n",
    "    model.parameters(),          # 最適化で更新対象のパラメーター（重みやバイアス）\n",
    "    lr=LEARNING_RATE,            # 更新時の学習率\n",
    "    weight_decay=REGULARIZATION) # L2正則化（※不要な場合は0か省略）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数（学習方法設計時に必要となるもの）\n",
    "criterion = nn.MSELoss()  # 損失関数：平均二乗誤差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_X, train_y):\n",
    "    # 訓練モードに設定\n",
    "    model.train()\n",
    "\n",
    "    # フォワードプロパゲーションで出力結果を取得\n",
    "    #train_X                # 入力データ\n",
    "    pred_y = model(train_X) # 出力結果\n",
    "    #train_y                # 正解ラベル\n",
    "\n",
    "    # 出力結果と正解ラベルから損失を計算し、勾配を求める\n",
    "    optimizer.zero_grad()   # 勾配を0で初期化（※累積してしまうため要注意）\n",
    "    loss = criterion(pred_y, train_y)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n",
    "    loss.backward()   # 逆伝播の処理として勾配を計算（自動微分）\n",
    "\n",
    "    # 勾配を使ってパラメーター（重みとバイアス）を更新\n",
    "    optimizer.step()  # 指定されたデータ分の最適化を実施\n",
    "\n",
    "    # 正解率を算出\n",
    "    with torch.no_grad(): # 勾配は計算しないモードにする\n",
    "        discr_y = discretize(pred_y)         # 確率値から「-1」／「1」に変換\n",
    "        acc = (discr_y == train_y).sum()     # 正解数を計算する\n",
    "\n",
    "    # 損失と正解数をタプルで返す\n",
    "    return (loss.item(), acc.item())  # ※item()=Pythonの数値\n",
    "\n",
    "def valid_step(valid_X, valid_y):\n",
    "    # 評価モードに設定（※dropoutなどの挙動が評価用になる）\n",
    "    model.eval()\n",
    "    \n",
    "    # フォワードプロパゲーションで出力結果を取得\n",
    "    #valid_X                # 入力データ\n",
    "    pred_y = model(valid_X) # 出力結果\n",
    "    #valid_y                # 正解ラベル\n",
    "\n",
    "    # 出力結果と正解ラベルから損失を計算\n",
    "    loss = criterion(pred_y, valid_y)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n",
    "    # ※評価時は勾配を計算しない\n",
    "\n",
    "    # 正解率を算出\n",
    "    with torch.no_grad(): # 勾配は計算しないモードにする\n",
    "        discr_y = discretize(pred_y)     # 確率値から「-1」／「1」に変換\n",
    "        acc = (discr_y == valid_y).sum() # 正解数を合計する\n",
    "\n",
    "    # 損失と正解数をタプルで返す\n",
    "    return (loss.item(), acc.item())  # ※item()=Pythonの数値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   1/100] loss: 27.14700, acc: 0.49600 val_loss: 21.71822, val_acc: 0.55200\n",
      "[Epoch   2/100] loss: 9.14963, acc: 0.61600 val_loss: 6.92132, val_acc: 0.66000\n",
      "[Epoch   3/100] loss: 4.47398, acc: 0.78400 val_loss: 3.88712, val_acc: 0.82800\n",
      "[Epoch   4/100] loss: 2.90991, acc: 0.80000 val_loss: 2.75248, val_acc: 0.83200\n",
      "[Epoch   5/100] loss: 2.13406, acc: 0.86000 val_loss: 2.00672, val_acc: 0.82800\n",
      "[Epoch   6/100] loss: 1.70766, acc: 0.81600 val_loss: 1.56362, val_acc: 0.84800\n",
      "[Epoch   7/100] loss: 1.38308, acc: 0.78400 val_loss: 1.29081, val_acc: 0.85200\n",
      "[Epoch   8/100] loss: 1.16109, acc: 0.81200 val_loss: 1.17332, val_acc: 0.81600\n",
      "[Epoch   9/100] loss: 0.98018, acc: 0.83600 val_loss: 0.94478, val_acc: 0.86000\n",
      "[Epoch  10/100] loss: 0.83432, acc: 0.83600 val_loss: 0.85878, val_acc: 0.84800\n",
      "[Epoch  11/100] loss: 0.74450, acc: 0.85200 val_loss: 0.72538, val_acc: 0.84400\n",
      "[Epoch  12/100] loss: 0.66778, acc: 0.85200 val_loss: 0.63853, val_acc: 0.87600\n",
      "[Epoch  13/100] loss: 0.62353, acc: 0.82000 val_loss: 0.56547, val_acc: 0.88800\n",
      "[Epoch  14/100] loss: 0.50753, acc: 0.88800 val_loss: 0.51097, val_acc: 0.87200\n",
      "[Epoch  15/100] loss: 0.45672, acc: 0.88800 val_loss: 0.45004, val_acc: 0.88800\n",
      "[Epoch  16/100] loss: 0.41117, acc: 0.89600 val_loss: 0.38951, val_acc: 0.90800\n",
      "[Epoch  17/100] loss: 0.36166, acc: 0.90000 val_loss: 0.33706, val_acc: 0.92800\n",
      "[Epoch  18/100] loss: 0.29942, acc: 0.93200 val_loss: 0.27736, val_acc: 0.93200\n",
      "[Epoch  19/100] loss: 0.25986, acc: 0.94800 val_loss: 0.25054, val_acc: 0.97600\n",
      "[Epoch  20/100] loss: 0.22931, acc: 0.98000 val_loss: 0.20568, val_acc: 0.99200\n",
      "[Epoch  21/100] loss: 0.19442, acc: 0.98800 val_loss: 0.19753, val_acc: 0.98800\n",
      "[Epoch  22/100] loss: 0.17756, acc: 0.99200 val_loss: 0.15932, val_acc: 0.99600\n",
      "[Epoch  23/100] loss: 0.15459, acc: 1.00000 val_loss: 0.14161, val_acc: 1.00000\n",
      "[Epoch  24/100] loss: 0.13685, acc: 1.00000 val_loss: 0.13218, val_acc: 0.99200\n",
      "[Epoch  25/100] loss: 0.12666, acc: 0.99600 val_loss: 0.11430, val_acc: 1.00000\n",
      "[Epoch  26/100] loss: 0.11108, acc: 1.00000 val_loss: 0.10313, val_acc: 1.00000\n",
      "[Epoch  27/100] loss: 0.10169, acc: 1.00000 val_loss: 0.09773, val_acc: 1.00000\n",
      "[Epoch  28/100] loss: 0.09387, acc: 1.00000 val_loss: 0.09061, val_acc: 1.00000\n",
      "[Epoch  29/100] loss: 0.08773, acc: 1.00000 val_loss: 0.08310, val_acc: 1.00000\n",
      "[Epoch  30/100] loss: 0.07974, acc: 0.99600 val_loss: 0.07526, val_acc: 1.00000\n",
      "[Epoch  31/100] loss: 0.07246, acc: 1.00000 val_loss: 0.07390, val_acc: 1.00000\n",
      "[Epoch  32/100] loss: 0.06913, acc: 1.00000 val_loss: 0.06518, val_acc: 1.00000\n",
      "[Epoch  33/100] loss: 0.06452, acc: 1.00000 val_loss: 0.06441, val_acc: 0.99600\n",
      "[Epoch  34/100] loss: 0.06143, acc: 0.99600 val_loss: 0.05647, val_acc: 1.00000\n",
      "[Epoch  35/100] loss: 0.05671, acc: 1.00000 val_loss: 0.05441, val_acc: 1.00000\n",
      "[Epoch  36/100] loss: 0.05349, acc: 1.00000 val_loss: 0.05368, val_acc: 1.00000\n",
      "[Epoch  37/100] loss: 0.05129, acc: 1.00000 val_loss: 0.04874, val_acc: 1.00000\n",
      "[Epoch  38/100] loss: 0.04642, acc: 1.00000 val_loss: 0.04927, val_acc: 1.00000\n",
      "[Epoch  39/100] loss: 0.04622, acc: 1.00000 val_loss: 0.04472, val_acc: 1.00000\n",
      "[Epoch  40/100] loss: 0.04415, acc: 1.00000 val_loss: 0.04071, val_acc: 1.00000\n",
      "[Epoch  41/100] loss: 0.04112, acc: 1.00000 val_loss: 0.04149, val_acc: 1.00000\n",
      "[Epoch  42/100] loss: 0.04025, acc: 1.00000 val_loss: 0.03769, val_acc: 1.00000\n",
      "[Epoch  43/100] loss: 0.03777, acc: 1.00000 val_loss: 0.03733, val_acc: 1.00000\n",
      "[Epoch  44/100] loss: 0.03620, acc: 1.00000 val_loss: 0.03568, val_acc: 1.00000\n",
      "[Epoch  45/100] loss: 0.03574, acc: 1.00000 val_loss: 0.03321, val_acc: 1.00000\n",
      "[Epoch  46/100] loss: 0.03326, acc: 1.00000 val_loss: 0.03193, val_acc: 1.00000\n",
      "[Epoch  47/100] loss: 0.03231, acc: 1.00000 val_loss: 0.03162, val_acc: 1.00000\n",
      "[Epoch  48/100] loss: 0.03149, acc: 1.00000 val_loss: 0.02962, val_acc: 1.00000\n",
      "[Epoch  49/100] loss: 0.03028, acc: 1.00000 val_loss: 0.02877, val_acc: 1.00000\n",
      "[Epoch  50/100] loss: 0.02880, acc: 1.00000 val_loss: 0.02772, val_acc: 1.00000\n",
      "[Epoch  51/100] loss: 0.02886, acc: 1.00000 val_loss: 0.02697, val_acc: 1.00000\n",
      "[Epoch  52/100] loss: 0.02702, acc: 1.00000 val_loss: 0.02622, val_acc: 1.00000\n",
      "[Epoch  53/100] loss: 0.02672, acc: 1.00000 val_loss: 0.02528, val_acc: 1.00000\n",
      "[Epoch  54/100] loss: 0.02510, acc: 1.00000 val_loss: 0.02685, val_acc: 1.00000\n",
      "[Epoch  55/100] loss: 0.02515, acc: 1.00000 val_loss: 0.02400, val_acc: 1.00000\n",
      "[Epoch  56/100] loss: 0.02384, acc: 1.00000 val_loss: 0.02344, val_acc: 1.00000\n",
      "[Epoch  57/100] loss: 0.02337, acc: 1.00000 val_loss: 0.02471, val_acc: 1.00000\n",
      "[Epoch  58/100] loss: 0.02272, acc: 1.00000 val_loss: 0.02318, val_acc: 1.00000\n",
      "[Epoch  59/100] loss: 0.02277, acc: 1.00000 val_loss: 0.02241, val_acc: 1.00000\n",
      "[Epoch  60/100] loss: 0.02239, acc: 1.00000 val_loss: 0.02142, val_acc: 1.00000\n",
      "[Epoch  61/100] loss: 0.02176, acc: 1.00000 val_loss: 0.02408, val_acc: 1.00000\n",
      "[Epoch  62/100] loss: 0.02090, acc: 1.00000 val_loss: 0.02144, val_acc: 1.00000\n",
      "[Epoch  63/100] loss: 0.02091, acc: 1.00000 val_loss: 0.02133, val_acc: 1.00000\n",
      "[Epoch  64/100] loss: 0.02014, acc: 1.00000 val_loss: 0.01946, val_acc: 1.00000\n",
      "[Epoch  65/100] loss: 0.02024, acc: 1.00000 val_loss: 0.01976, val_acc: 1.00000\n",
      "[Epoch  66/100] loss: 0.01918, acc: 1.00000 val_loss: 0.01961, val_acc: 1.00000\n",
      "[Epoch  67/100] loss: 0.01840, acc: 1.00000 val_loss: 0.01877, val_acc: 1.00000\n",
      "[Epoch  68/100] loss: 0.01865, acc: 1.00000 val_loss: 0.01874, val_acc: 1.00000\n",
      "[Epoch  69/100] loss: 0.01884, acc: 1.00000 val_loss: 0.01793, val_acc: 1.00000\n",
      "[Epoch  70/100] loss: 0.01780, acc: 1.00000 val_loss: 0.01750, val_acc: 1.00000\n",
      "[Epoch  71/100] loss: 0.01734, acc: 1.00000 val_loss: 0.01758, val_acc: 1.00000\n",
      "[Epoch  72/100] loss: 0.01753, acc: 1.00000 val_loss: 0.01704, val_acc: 1.00000\n",
      "[Epoch  73/100] loss: 0.01711, acc: 1.00000 val_loss: 0.01688, val_acc: 1.00000\n",
      "[Epoch  74/100] loss: 0.01680, acc: 1.00000 val_loss: 0.01622, val_acc: 1.00000\n",
      "[Epoch  75/100] loss: 0.01586, acc: 1.00000 val_loss: 0.01636, val_acc: 1.00000\n",
      "[Epoch  76/100] loss: 0.01612, acc: 1.00000 val_loss: 0.01609, val_acc: 1.00000\n",
      "[Epoch  77/100] loss: 0.01652, acc: 1.00000 val_loss: 0.01732, val_acc: 1.00000\n",
      "[Epoch  78/100] loss: 0.01610, acc: 1.00000 val_loss: 0.01542, val_acc: 1.00000\n",
      "[Epoch  79/100] loss: 0.01507, acc: 1.00000 val_loss: 0.01500, val_acc: 1.00000\n",
      "[Epoch  80/100] loss: 0.01491, acc: 1.00000 val_loss: 0.01637, val_acc: 1.00000\n",
      "[Epoch  81/100] loss: 0.01454, acc: 1.00000 val_loss: 0.01446, val_acc: 1.00000\n",
      "[Epoch  82/100] loss: 0.01450, acc: 1.00000 val_loss: 0.01467, val_acc: 1.00000\n",
      "[Epoch  83/100] loss: 0.01433, acc: 1.00000 val_loss: 0.01476, val_acc: 1.00000\n",
      "[Epoch  84/100] loss: 0.01435, acc: 1.00000 val_loss: 0.01432, val_acc: 1.00000\n",
      "[Epoch  85/100] loss: 0.01422, acc: 0.99600 val_loss: 0.01380, val_acc: 1.00000\n",
      "[Epoch  86/100] loss: 0.01402, acc: 1.00000 val_loss: 0.01471, val_acc: 1.00000\n",
      "[Epoch  87/100] loss: 0.01359, acc: 1.00000 val_loss: 0.01375, val_acc: 1.00000\n",
      "[Epoch  88/100] loss: 0.01379, acc: 1.00000 val_loss: 0.01321, val_acc: 1.00000\n",
      "[Epoch  89/100] loss: 0.01303, acc: 1.00000 val_loss: 0.01301, val_acc: 1.00000\n",
      "[Epoch  90/100] loss: 0.01296, acc: 1.00000 val_loss: 0.01388, val_acc: 1.00000\n",
      "[Epoch  91/100] loss: 0.01263, acc: 1.00000 val_loss: 0.01343, val_acc: 1.00000\n",
      "[Epoch  92/100] loss: 0.01246, acc: 1.00000 val_loss: 0.01277, val_acc: 1.00000\n",
      "[Epoch  93/100] loss: 0.01261, acc: 1.00000 val_loss: 0.01481, val_acc: 1.00000\n",
      "[Epoch  94/100] loss: 0.01227, acc: 1.00000 val_loss: 0.01252, val_acc: 1.00000\n",
      "[Epoch  95/100] loss: 0.01206, acc: 1.00000 val_loss: 0.01293, val_acc: 1.00000\n",
      "[Epoch  96/100] loss: 0.01233, acc: 1.00000 val_loss: 0.01373, val_acc: 1.00000\n",
      "[Epoch  97/100] loss: 0.01181, acc: 1.00000 val_loss: 0.01297, val_acc: 1.00000\n",
      "[Epoch  98/100] loss: 0.01245, acc: 1.00000 val_loss: 0.01225, val_acc: 1.00000\n",
      "[Epoch  99/100] loss: 0.01190, acc: 1.00000 val_loss: 0.01237, val_acc: 1.00000\n",
      "[Epoch 100/100] loss: 0.01142, acc: 1.00000 val_loss: 0.01210, val_acc: 1.00000\n",
      "Finished Training\n",
      "OrderedDict([('layer1.weight', tensor([[ 0.4851, -0.2407],\n",
      "        [-0.4718, -0.3392],\n",
      "        [-0.0013,  0.5468]])), ('layer1.bias', tensor([1.0422, 1.0698, 1.0133])), ('layer2.weight', tensor([[ 0.8102,  0.7564,  0.7047],\n",
      "        [ 0.5876,  0.7578,  0.6953],\n",
      "        [-0.4252,  0.1028, -0.2996]])), ('layer2.bias', tensor([-0.9635, -0.8280,  0.0201])), ('layer_out.weight', tensor([[ 1.7193,  1.5071, -0.4095]])), ('layer_out.bias', tensor([-0.5168]))])\n"
     ]
    }
   ],
   "source": [
    "# パラメーター（重みやバイアス）の初期化を行う関数の定義\n",
    "def init_parameters(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(layer.weight) # 重みを「一様分布のランダム値」に初期化\n",
    "        layer.bias.data.fill_(0.0)            # バイアスを「0」に初期化\n",
    "\n",
    "# 学習の前にパラメーター（重みやバイアス）を初期化する\n",
    "model.apply(init_parameters)\n",
    "\n",
    "# 定数（学習／評価時に必要となるもの）\n",
    "EPOCHS = 100             # エポック数： 100\n",
    "\n",
    "# 変数（学習／評価時に必要となるもの）\n",
    "avg_loss = 0.0           # 「訓練」用の平均「損失値」\n",
    "avg_acc = 0.0            # 「訓練」用の平均「正解率」\n",
    "avg_val_loss = 0.0       # 「評価」用の平均「損失値」\n",
    "avg_val_acc = 0.0        # 「評価」用の平均「正解率」\n",
    "\n",
    "# 損失の履歴を保存するための変数\n",
    "train_history = []\n",
    "valid_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # forループ内で使う変数と、エポックごとの値リセット\n",
    "    total_loss = 0.0     # 「訓練」時における累計「損失値」\n",
    "    total_acc = 0.0      # 「訓練」時における累計「正解数」\n",
    "    total_val_loss = 0.0 # 「評価」時における累計「損失値」\n",
    "    total_val_acc = 0.0  # 「評価」時における累計「正解数」\n",
    "    total_train = 0      # 「訓練」時における累計「データ数」\n",
    "    total_valid = 0      # 「評価」時における累計「データ数」\n",
    "\n",
    "    for train_X, train_y in loader_train:\n",
    "        # 【重要】1ミニバッチ分の「訓練」を実行\n",
    "        loss, acc = train_step(train_X, train_y)\n",
    "\n",
    "        # 取得した損失値と正解率を累計値側に足していく\n",
    "        total_loss += loss          # 訓練用の累計損失値\n",
    "        total_acc += acc            # 訓練用の累計正解数\n",
    "        total_train += len(train_y) # 訓練データの累計数\n",
    "            \n",
    "    for valid_X, valid_y in loader_valid:\n",
    "        # 【重要】1ミニバッチ分の「評価（精度検証）」を実行\n",
    "        val_loss, val_acc = valid_step(valid_X, valid_y)\n",
    "\n",
    "        # 取得した損失値と正解率を累計値側に足していく\n",
    "        total_val_loss += val_loss  # 評価用の累計損失値\n",
    "        total_val_acc += val_acc    # 評価用の累計正解数\n",
    "        total_valid += len(valid_y) # 訓練データの累計数\n",
    "\n",
    "    # ミニバッチ単位で累計してきた損失値や正解率の平均を取る\n",
    "    n = epoch + 1                             # 処理済みのエポック数\n",
    "    avg_loss = total_loss / n                 # 訓練用の平均損失値\n",
    "    avg_acc = total_acc / total_train         # 訓練用の平均正解率\n",
    "    avg_val_loss = total_val_loss / n         # 訓練用の平均損失値\n",
    "    avg_val_acc = total_val_acc / total_valid # 訓練用の平均正解率\n",
    "\n",
    "    # グラフ描画のために損失の履歴を保存する\n",
    "    train_history.append(avg_loss)\n",
    "    valid_history.append(avg_val_loss)\n",
    "\n",
    "    # 損失や正解率などの情報を表示\n",
    "    print(f'[Epoch {epoch+1:3d}/{EPOCHS:3d}]' \\\n",
    "          f' loss: {avg_loss:.5f}, acc: {avg_acc:.5f}' \\\n",
    "          f' val_loss: {avg_val_loss:.5f}, val_acc: {avg_val_acc:.5f}')\n",
    "\n",
    "print('Finished Training')\n",
    "print(model.state_dict())  # 学習後のパラメーターの情報を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 学習結果（損失）のグラフを描画\n",
    "epochs = len(train_history)\n",
    "plt.plot(range(epochs), train_history, marker='.', label='loss (Training data)')\n",
    "plt.plot(range(epochs), valid_history, marker='.', label='loss (Validation data)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
